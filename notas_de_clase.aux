\relax 
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducci\IeC {\'o}n}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Or\IeC {\'\i }genes: Inteligencia Artificial}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Breve historia del aprendizaje de m\IeC {\'a}quinas}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Taxonom\IeC {\'\i }a del aprendizaje de m\IeC {\'a}quinas}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Relaci\IeC {\'o}n con otras disciplinas}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Estado del aprendizaje de m\IeC {\'a}quinas y desaf\IeC {\'\i }os}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Regresi\IeC {\'o}n Lineal}{12}}
\newlabel{eq:training_set}{{1}{12}}
\newlabel{eq:reg_lin_fn}{{2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}M\IeC {\'\i }nimos cuadrados}{12}}
\newlabel{ssub:min_cuad}{{2.1}{12}}
\newlabel{eq:least_squares}{{3}{12}}
\newlabel{eq:lin_least_squares}{{4}{12}}
\newlabel{eq:truco_reg_lin}{{5}{12}}
\newlabel{eq:lin_least_squares2}{{6}{13}}
\newlabel{eq:sol_mse}{{7}{13}}
\newlabel{eq:matrices_X}{{8}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ejemplo de regresi\IeC {\'o}n lineal.\relax }}{13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reg_lin_1}{{1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Interpretaci\IeC {\'o}n geom\IeC {\'e}trica de la regresi\IeC {\'o}n lineal\relax }}{14}}
\newlabel{fig:projection}{{2}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}M\IeC {\'\i }nimos cuadrados regularizados}{14}}
\newlabel{ssub:min_cuad_reg}{{2.1.1}{14}}
\newlabel{eq:reg_least_squares}{{9}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}M\IeC {\'a}xima verosimilitud}{15}}
\newlabel{ssub:max_ver}{{2.1.2}{15}}
\newlabel{eq:lin_gauss}{{15}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Regresi\IeC {\'o}n Bayesiana}{16}}
\newlabel{ssub:map}{{2.1.3}{16}}
\newlabel{eq:posterior}{{23}{16}}
\newlabel{eq:posterior2}{{25}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Resumen de las probabilidades para el caso del test de cancer.\relax }}{18}}
\newlabel{table:cancer}{{3}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Maximo a posteriori}{19}}
\newlabel{sub:map}{{2.2}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Predicciones}{19}}
\newlabel{sub:predicciones}{{2.3}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Inference beyond supervised learning}{19}}
\newlabel{sub:inference_beyond_supervised_learning}{{2.4}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Regresi\IeC {\'o}n No Lineal}{20}}
\newlabel{eq:training_set_nl}{{51}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bases de funciones}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Clasificaci\IeC {\'o}n Lineal}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Caso 2 clases}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ejemplo de datos, los puntos  en azul pertenecen a $\mathcal  {C}_1$, los rojos a $\mathcal  {C}_2$.\relax }}{24}}
\newlabel{fig:puntos_2d}{{4}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Caso multi clase}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Clasificaci\IeC {\'o}n con m\IeC {\'\i }nimos cuadrados}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Ejemplo ilustrativo sobre como los puntos lejanos pueden afectar negativamente los resultados.\relax }}{27}}
\newlabel{fig:clasif_mse}{{5}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Discriminante lineal de Fisher}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (a) Resultado obtenido al proyectar sobre la recta que une los promedios de cada clase. (b) Resultado al considerar una correcci\IeC {\'o}n que considera minimizar la dispersi\IeC {\'o}n entre clases.\relax }}{28}}
\newlabel{fig:ej_fda}{{6}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Clasificaci\IeC {\'o}n No Lineal}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}El Perceptr\IeC {\'o}n}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Clasificaci\IeC {\'o}n Probabilista}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Regresi\IeC {\'o}n Log\IeC {\'\i }stica v/s Modelo Generativo}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Selecci\IeC {\'o}n de Modelo}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ejemplos de sub, sobre y correcto ajuste.\relax }}{35}}
\newlabel{fig:overfitting}{{7}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Criterio de Informaci\IeC {\'o}n Akaike}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Criterio de Informaci\IeC {\'o}n Bayesiano}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluaci\IeC {\'o}n y comparaci\IeC {\'o}n de modelos}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Error cuadr\IeC {\'a}tico medio}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}log-densidad predictiva o log-verosimilitud}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Otros m\IeC {\'e}todos}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Promedio de Modelos}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Elecci\IeC {\'o}n de pesos mediante softmax}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Bayesian Model Averaring}{40}}
\newlabel{eq:lambda}{{179}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Redes Neuronales - Editado}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Introducci\'on y Arquitectura}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Conceptos B\'asicos}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ejemplo de una red neuronal de 1 capa: (\textit  {izquierda}) Se muestra la red con inputs $x_1$ y $x_2$, que luego pasan a la capa oculta para producir el output $y$. (\textit  {derecha}) Misma red con una representaci\'on vectorial de las capas. La matriz ${\bm  {W}}$ describe el mapping de ${\bm  {x}}$ a ${\bm  {h}}$, y la matriz $w$ el mapping de ${\bm  {h}}$ a $y$.\relax }}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Funci\'on de Costos, Unidades de Output y la Formulaci\'on como un Problema Probabil\'istico}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Estructura Interna de la Red}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Dise\~{n}o de Arquitectura y Teorema de Aproximaci\'on Universal}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Entrenamiento de una Red Neuronal}{45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Forward Propagation y Back-Propagation}{45}}
\newlabel{BP_beta}{{195}{47}}
\newlabel{BP_alfa}{{200}{47}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward Propagation\relax }}{48}}
\newlabel{ML:Algorithm1}{{1}{48}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Back-Propagation\relax }}{48}}
\newlabel{ML:Algorithm2}{{2}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Regularizaci\'on para una Red Neuronal}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Regularizaci\'on $\bm  {L}^{2}$}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Dropout}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Dropout entrena potencialemente todas las subredes que se puedan formar a partir de la red neuronal original (primer recuadro) al apagar el output que producen las distintas unidades\relax }}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Otros M\'etodos de Regularizaci\'on}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Algoritmos de Optimizaci\'on}{51}}
\newlabel{gradiente_completo}{{205}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Descenso del Gradiente Estoc\'astico y por Batches}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Algoritmos con Momentum}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Algoritmos con Learning Rates Adaptativos}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Deep Learning y Otros Tipos de Redes Neuronales}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Redes Neuronales Convolucionales}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Capa de una red convolucional\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Convoluci\'on con un stride igual a 2\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Efecto de no usar zero-padding en una red convolucional (Arriba) y efecto de usar zero padding en una red convolucional (Abajo) en cuanto al tama{\~{n}}o de la red\relax }}{55}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Redes Neuronales Recurrentes}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Ejemplo de una red recurrente sin output\relax }}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Ejemplo de una red recurrente que produce un output en cada instante de tiempo, y que comparte el estado a trav\'es del tiempo\relax }}{57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Autoencoders}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Estructura de un \textit  {autoencoder} t\'ipico\relax }}{58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Redes Generativas Adversariales}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Im\'agenes generadas por una GAN entrenada con el set de datos LSUN. (Izquierda) Im\'agenes de dormitorios generadas por el modelo DCGAN (imagen de Radford et al., 2015). (Derecha) Im\'agenes de iglesias generadas por el modelo LAPGAN (imagen de Denton et al., 2015)\relax }}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Support Vector Machines}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Introducci\IeC {\'o}n}{60}}
\newlabel{sub:svm_intro}{{6.1}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Idea general}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Modelo de m\IeC {\'a}ximo margen entre los datos\relax }}{60}}
\newlabel{fig:maxim_marg}{{17}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Los vectores de soporte son la manzana m\IeC {\'a}s naranjezca y la naranja m\IeC {\'a}s manzanezca\relax }}{61}}
\newlabel{fig:my_label1}{{18}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Problema}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces En azul los vectores $x_+$ y $x_{-}$. En rojo el vector $(x_+ - x_{-})$. En naranjo la componente del vector rojo, proyectada en la direcci\IeC {\'o}n definda por $\frac  {w}{||w||}$. Notar que corresponde justamente al doble del margen\relax }}{62}}
\newlabel{fig:my_label2}{{19}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Soft Margin}{63}}
\newlabel{fig:my_label3}{{\caption@xref {fig:my_label3}{ on input line 162}}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Visualizaci\IeC {\'o}n de los valores de $\xi $\relax }}{64}}
\newlabel{fig:my_label4}{{20}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Kernel Methods}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Datos XOR no son linealmente separables\relax }}{65}}
\newlabel{fig:my_label5}{{21}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Los puntos rojos y azules corresponden a los datos mapeados a trav\IeC {\'e}s de $\phi $. El plano $z=0$ es claramente capaz de separar puntos rojos y puntos auzles en este nuevo espacio.\relax }}{66}}
\newlabel{fig:my_label6}{{22}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Ejemplo}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Kernel SVM}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Procesos Gaussianos}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Muestreo de un prior $\ensuremath  {\mathcal  {GP}}$}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Muestras de un prior $\ensuremath  {\mathcal  {GP}}$ con kernel SE, para distintos \textit  {lenghtscales} ($\ell $) y funci\IeC {\'o}n media $m(\cdot )=0$, la parte sombreada corresponde al intervalo de confianza del $95\%$. Se puede ver que a mayor $\ell $ las funciones se van volviendo m\IeC {\'a}s suaves.\relax }}{72}}
\newlabel{fig:gp_1}{{23}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Incorporando Informaci\IeC {\'o}n: Evaluaci\IeC {\'o}n sin ruido}{72}}
\newlabel{eq:gp_post}{{227}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Regresi\IeC {\'o}n con $\ensuremath  {\mathcal  {GP}}$ para se\IeC {\~n}al sintetica usando el 15$\%$ de los datos muestreados de forma no uniforme, utilizand un $\ensuremath  {\mathcal  {GP}}$ de media nula y kernel SE.\relax }}{73}}
\newlabel{fig:gp_2}{{24}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Incorporando Informaci\IeC {\'o}n: Evaluaci\IeC {\'o}n con ruido}{73}}
\newlabel{eq:gp_posterior}{{232}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Regresi\IeC {\'o}n con $\ensuremath  {\mathcal  {GP}}$ para se\IeC {\~n}al sintetica usando el 15$\%$ de los datos muestreados de forma no uniforme y contaminados con ruido Gaussiano, utilizando un $\ensuremath  {\mathcal  {GP}}$ de media nula y kernel SE.\relax }}{74}}
\newlabel{fig:gp_3}{{25}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Entrenamiento y optimizaci\IeC {\'o}n de un $\ensuremath  {\mathcal  {GP}}$}{74}}
\newlabel{eq:gp_nll}{{237}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Log verosimilitud marginal negativa (NLL) en funci\IeC {\'o}n del \textit  {lengthscale} ($\ell $) para se\IeC {\~n}al sintetica, se mantienen constantes los otros par\IeC {\'a}metros del $\ensuremath  {\mathcal  {GP}}$.\relax }}{75}}
\newlabel{fig:gp_4}{{26}{75}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Resultado optimizaci\IeC {\'o}n $\ensuremath  {\mathcal  {GP}}$ y comparaci\IeC {\'o}n con los par\IeC {\'a}metros sin entrenar.\relax }}{76}}
\newlabel{tab:gp_1}{{1}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Regresi\IeC {\'o}n con $\ensuremath  {\mathcal  {GP}}$ para se\IeC {\~n}al sintetica usando el 15$\%$ de los datos muestreados de forma no uniforme y contaminados con ruido Gaussiano, utilizando un $\ensuremath  {\mathcal  {GP}}$ de media nula y kernel SE; Modelo optimizado utilizando L-BFGS.\relax }}{77}}
\newlabel{fig:gp_5}{{27}{77}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Complejidad Computacional}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Funciones de covarianza (Kernels)}{77}}
\newlabel{eq:gp_kernel_se}{{238}{78}}
\newlabel{eq:gp_kernel_rq}{{239}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Kernel \textit  {Rational Quadratic}, en la izquierda se muestra la covarianza en funci\IeC {\'o}n de su argumento $\tau =x-x'$, a la derecha de un $\ensuremath  {\mathcal  {GP}}$ usando un kernel RQ.\relax }}{78}}
\newlabel{fig:gp_6}{{28}{78}}
\newlabel{eq:gp_kernel_p}{{240}{79}}
\newlabel{eq:gp_kernel_lp}{{241}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Kernel peri\IeC {\'o}dico, en la izquierda se muestra la covarianza en funci\IeC {\'o}n de su argumento $\tau =x-x'$, a la derecha de un $\ensuremath  {\mathcal  {GP}}$ usando un kernel peri\IeC {\'o}dico.\relax }}{79}}
\newlabel{fig:gp_7}{{29}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Operaciones con kernels}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Representaci\IeC {\'o}n espectral}{79}}
\newlabel{eq:gp_spectral}{{242}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Extensiones para un $\ensuremath  {\mathcal  {GP}}$}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}$\ensuremath  {\mathcal  {GP}}$ de clasificaci\IeC {\'o}n}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces $\ensuremath  {\mathcal  {GP}}$ de clasificaci\IeC {\'o}n utilizando datos sint\IeC {\'e}ticos. Este clasificador entrega una densidad de probabilidad en vez de una sola funci\IeC {\'o}n de decisi\IeC {\'o}n.\relax }}{80}}
\newlabel{fig:gp_8}{{30}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.2}Selecci\IeC {\'o}n autom\IeC {\'a}tica de relevancia (ARD) (\textit  {Selecci\IeC {\'o}n autom\IeC {\'a}tica de features})}{80}}
\newlabel{eq:gp_ard}{{243}{80}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.3}Multi output $\ensuremath  {\mathcal  {GP}}$}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Diferentes Interpretaciones de un $\ensuremath  {\mathcal  {GP}}$}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.1}De regresi\IeC {\'o}n lineal a $\ensuremath  {\mathcal  {GP}}$}{81}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.7.2}Nota sobre RKHS}{82}}
\newlabel{eq:gp_nll_summary}{{255}{83}}
\newlabel{eq:gp_posterior_summary}{{257}{83}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Aprendizaje No Supervisado}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Reducci\IeC {\'o}n de dimensionalidad}{84}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Principal Component Analysis (PCA)}{84}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Kernel PCA}{84}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Ejemplo en que kernel PCA sobre un conjunto de datos que no es linealmente separable.\relax }}{85}}
\newlabel{fig:kpca}{{31}{85}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Probabilistic PCA}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Clustering}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}k-means}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Gaussian Mixture Model}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces (a) Datos reales con sus etiquetas correctas. (b) Clusters encontrados por k-means.\relax }}{88}}
\newlabel{fig:kmeans}{{32}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces (a) Datos reales con sus etiquetas correctas. (b) Clusters encontrados por GMM.\relax }}{89}}
\newlabel{fig:gmm}{{33}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Density-based spatial clustering of applications with noise (DBSCAN)}{89}}
\newlabel{DBSCAN}{{3}{90}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Pseudo c\IeC {\'o}digo de DBSCAN \relax }}{90}}
\newlabel{alg:expandirCluster}{{4}{90}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Funci\IeC {\'o}n para expandir cluster. \relax }}{90}}
\newlabel{alg:regionDeConsulta}{{5}{90}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Retorna los puntos de la vecindad de b\IeC {\'u}squeda para un punto. \relax }}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces (a) Datos reales con sus etiquetas correctas. (b) Clusters encontrados por DBSCAN.\relax }}{91}}
\newlabel{fig:dbscan}{{34}{91}}
