%!TEX root = ../notas_de_clase.tex


\section{Introducción}


El aprendizaje de máquinas (AM) es una disciplina que reúne elementos de ciencias de la computación, optimización, estadística, probabilidades y ciencias cognitivas para construir el motor de aprendizaje dentro de la Inteligencia Artificial. Definido por Arthur Samuel en 1950, el AM es la disciplina que da a las máquinas la habilidad de aprender sin ser explícitamente programadas. Si bien existen enfoques al AM inspirados en sistemas biológicos, esta no es la única forma de construir métodos de aprendizaje: una analogía se puede identificar en los primeros intentos por construir máquinas voladoras, en donde se pretendía replicar el movimiento de las alas de un pájaro, sin embargo, estos intentos no fueron exitosos y no fue sino hasta la invención del primer avión, el cual no mueve sus alas, que el hombre logró construir la primera máquina voladora. Esto sugiere que el paradigma biológico no es exclusivo al momento de construir máquinas inteligentes que utilicen observaciones de su entorno para extraer información útil y generar predicciones. 


AM es una disciplina joven que ha experimentado un vertiginoso crecimiento en las últimas décadas, esto ha sido posible en gran parte gracias a los recientes avances computacionales y la cantidad de datos que permite entrenar modelos complejos. El uso masivo de técnicas AM se ha visto reflejado en distintas áreas que incluyen visión computacional, clasificación de secuencias de ADN, marketing, detección de fraude, diagnósticos médicos, análisis financiero y traducción de texto por nombrar algunas. Adicionalmente, si bien el objetivo de AM es desarrollar algoritmos de aprendizaje prescindiendo en gran medida de la intervención humana,  otra razón del éxito del AM es su facilidad para acoplarse con otras disciplinas aplicadas, en particular al área que hoy conocemos como \textit{Data Science}. 

\subsection{Orígenes: Inteligencia Artificial}

Nuestras habilidades cognitivas nos diferencian fuertemente del resto de las especies del reino animal y el dominio del \emph{Homo sapiens} sobre el planeta radica en estas habilidades: la inteligencia humana superior a la del resto de los animales nos permite adaptarnos a diferentes situaciones ambientales y sociológicas de forma rápida y sin la necesidad de un cambio evolutivo. Por ejemplo, para migrar desde África al norte de Europa y a Oceanía, el \emph{Homo sapiens} no se adaptó biológicamente a climas distintos, sino que manipuló herramientas y materiales para producir vestimenta adecuada y embarcaciones. Otra característica única del \emph{Homo sapiens} es su habilidad de creer en un imaginario colectivo que permite construir organizaciones con un gran número de individuos, lo cual nuevamente exclusiva a nuestra especie y consecuencia de nuestra inteligencia \cite{harari}. 

Nuestro interés en entender la inteligencia puede ser identificado desde los comienzos de la Filosofía y Sicología, disciplinas que se han dedicado al estudio de las forma en que entendemos, recordamos, razonamos y aprendemos. La inteligencia artificial (IA) es una disciplina mucho más reciente que las anteriores y va un paso más allá de la mera comprensión de la inteligencia, pues apunta a replicarla e incluso mejorarla \cite{bostrom_2014}. Hay varias definiciones de IA dependiendo de si (i) adoptamos un punto de vista de razonamiento versus comportamiento, o bien si (ii) identificamos las acciones inteligentes como humanas u objetivamente racionales, una de estas posiciones es la de Alan Turing, el que mediante el juego de la imitación \cite{turing_1950}, sentencia que una máquina es inteligente si es capaz de desarrollar tareas cognitivas a un nivel “humano” suficiente para engañar a un interrogador (también humano). En este contexto para que una máquina sea inteligente, o equivalentemente, apruebe el test de Turing, es necesario que posea \cite{russell_norvig_2009}:  
\begin{itemize}
	\item \textbf{procesamiento de lenguaje natural} para comunicarse con seres humanos, en particular, el interrogador,
	\item \textbf{representación del conocimiento} para guardar información recibida antes y durante la interrogación,
	\item \textbf{razonamiento automático} para usar la información guardada y formular respuestas y conclusiones, y
	\item \textbf{aprendizaje de máquinas} para adaptarse a nuevas circunstancias y descubrir patrones.
\end{itemize}

El test de Turing sigue siendo un tópico de investigación en Filosofía hasta el día de hoy, sin embargo, los avances actuales de la inteligencia artificial no están necesariamente enfocados en diseñar máquinas para aprobar dicho test. Si bien los inicios de la IA están en la Filosofía, actualmente los avances en IA apuntan a desarrollar metodologías para aprender de grandes volúmenes de información sin necesariamente actuar de forma humana, en particular, el componente de aprendizaje de máquinas en la lista anterior ha jugado un papel fundamental en esta nueva etapa, en donde su aporte en distintas áreas de aplicación es cada vez más evidente. 


\subsection{Breve historia del aprendizaje de máquinas}

En base a las definiciones de la inteligencia de máquinas asentadas por Turing en 1950, las primeras redes neuronales artificiales comenzaron a emerger en los trabajos seminales de \cite{minsky_1952} que programó el primer simulador de una red neuronal, \cite{farley_1954} que implementaron un algoritmo de prueba y error para el aprendizaje, y \cite{rosenblatt_1958} que propuso el Perceptrón. En los años siguientes la investigación en redes neuronales se vio afectada por las limitaciones de dichas estructuras expuestas en \cite{minsky_papert_1969} dando origen a lo que es conocido como el primer invierno de la inteligencia artificial, en donde el Profesor Sir James Lighthill expuso frente al parlamento inglés que la inteligencia artificial se fijaba objetivos no realistas y solo servía para escenarios básicos \cite{lighthill_1973}. Esta desconfianza en los alcances de la IA ralentizó su desarrollo, sin embargo, los conexionistas  seguirían investigando sobre formas de diseñar y entrenar redes neuronales, específicamente, los resultados de \cite{werbos_1974} que culminarían en el algoritmo de backpropagation propuesto por \cite{rumelhart_1986} y los avances de  \cite{hopfield_1982} en redes neuronales recurrentes permitirían terminar con el primer invierno de la inteligencia artificial.  
 
Durante el receso del conexionismo proliferaron los sistemas basados en reglas, en particular, los sistemas experto compuestos por una serie de reglas condicionales “si-entonces” (if-then) que replican el comportamiento de un humano experto, estos métodos se convirtieron en la primera herramienta exitosa de la IA en aplicaciones reales. Sin embargo, los sistemas experto no aprenden por sí solos, en el sentido de que las reglas si-entonces deben ser explícitamente programadas por un humano, esto tiene un costo considerable dependiendo de la complejidad del problema, por lo que hacia el comienzo de la década de los 90s los sistemas experto colapsaron debido a que la cantidad de información disponible aumentaba y dicho enfoque no es “escalable”. Un sistema basado en regla que aún se utiliza son los llamado árboles de decisión \cite{breiman_1984}, los cuales difieren de los sistemas experto en que las reglas no son definidas por un humano sino que descubiertas en base a la elección de variables que mejor segmentan los datos de forma supervisada.

Las redes neuronales vieron un resurgimiento en los 80s con el método de backpropagation que permitía entrenar redes neuronales de más de dos capas usando la regla de la cadena, esto permitió finalmente validar la premisa conexionista en tareas complejas, específicamente en reconocimiento de caracteres usando redes convolucionales como el Neocognitron \cite{fukushima_1980} y LeNet-5 \cite{lecun_1989}, y reconocimiento de voz usando redes neuronales recurrentes \cite{hopfield_1982}. Luego de esto vino una segunda caída del conexionismo hacia fines de los 80s, debido a inexistencia de una clara teoría que explicara el desempeño de las redes neuronales, su tendencia a sobreajustar y su elevado costo computacional. A principios de los 90s, y basado en la teoría del aprendizaje estadístico \cite{vapnik_1971}, surgieron los métodos basados en kernels, específicamente las máquinas de soporte vectorial (MSV) \cite{boser_1992}, esta nueva clase de algoritmos estaba fundamentada en una base teórica que combinaba elementos de estadística y análisis funcional para caracterizar los conceptos de sobreajuste, optimalidad de soluciones y funciones de costo en el contexto del aprendizaje de máquinas. Además de sus fundamentos teóricos, las MSV mostraron ser una alternativa competitiva a las redes neuronales en cuanto a su desempeño y costo computacional en distintas aplicaciones. 

También en la década de los 90, surgieron nuevos enfoques de aprendizaje de máquinas en donde el manejo de incertidumbre era abordado usando teoría de probabilidades, este es probablemente es punto de mayor similitud entre AM y Estadística \cite{ghahramani_2015}. Este enfoque procede definiendo una clase de modelos generativos probabilísticos, es decir, se asume que los datos observados son generados por un modelo incierto y aleatorio, luego, el aprendizaje consiste en convertir la “probabilidad de los datos dado el modelo” en la “probabilidad del modelo dado los datos” mediante el teorema de Bayes. Este es un enfoque elegante y teóricamente sustentado que permitió reinterpretar enfoques anteriores, sin embargo, muchas veces la formulación probabilística resulta en que las estimaciones del modelo y las predicciones no tienen forma explícita, es por esto que para el éxito del AM bayesiano fue necesario recurrir a técnicas de inferencia aproximada basadas en Monte Carlo \cite{neal_1993} o métodos variacionales \cite{jordan_1999}. El enfoque bayesiano permite definir todos los elementos del problema de aprendizaje (modelos, parámetros y predicciones) mediante distribuciones de probabilidad con la finalidad de caracterizar la incertidumbre en el modelo y definir intervalos de confianza en las predicciones, esto incluso permite hacer inferencia sobre modelos con infinitos parámetros \cite{hjort_2010}. En estos casos, el espacio de parámetros pueden ser todas las posible soluciones de un problema de aprendizaje, por ejemplo, el conjunto de las funciones continuas en regresión \cite{rasmussen_2006}, o el conjunto de todas las distribuciones de probabilidad en el caso de estimación de densidades \cite{ferguson_1973}.

Un nuevo resurgimiento de las redes neuronales (RN) se vio en los primeros años de la década del 2000, donde el área se renombró deep learning \cite{bengio_2009}. Progresivamente, el foco de la comunidad migró desde temáticas probabilistas o basadas en kernels para volver a RN pero ahora con un mayor número de capas. El éxito del enfoque conexionista finalmente logró objetivos propuestos hace décadas principalmente por dos factores: (i) la gran cantidad de datos disponibles, e.g., la base de datos ImageNet, y (ii) la gran capacidad computacional y paralelización del entrenamiento mediante el uso de tarjetas gráficas, esto permitió finalmente implementar RNs con billones de parámetros, las cuales eran complementadas con técnicas para evitar el sobreajuste. El hecho que los parámetros pierden significado para el entendimiento de las relaciones entre los datos aleja al AM de la Estadística, donde el objetivo es netamente predictivo y no la inferencia estadística: hasta el momento no hay otro enfoque que supere a deep learning en variadas aplicaciones, esto ha sido principalmente confirmado por los avances de Google, DeepMind y Facebook. De acuerdo a Max Welling, si bien la irrupción de deep learning aleja al AM de la Estadística, aún hay temáticas que las se nutren de ambas áreas como programación probabilista y computación bayesiana aproximada \cite{welling_2015}, adicionalmente, Yoshua Bengio cree que aún hay muchos aspectos inciertos de deep learning en los cuales los estadísticos podrían ayudar, tal como los especialistas de las ciencias de la computación se han dedicado a los aspectos estadísticos del aprendizaje de máquinas en el pasado \cite{bengio_2016}. 


\subsection{Taxonomía del aprendizaje de máquinas}

En la sección anterior se mencionaron distintos métodos de AM que son transversales a variados tipos de aprendizaje, en esta sección veremos los tres principales tipos de aprendizaje. Primero está el aprendizaje supervisado (AS), en donde los datos consisten en pares de la forma (dato, etiqueta) y el objetivo es estimar una función f(·) tal que etiqueta = f(dato) de acuerdo a cierta medida de rendimiento. El nombre supervisado viene del hecho que los datos disponibles están “etiquetados”, y por ende es posible supervisar el entrenamiento del algoritmo. Ejemplos de AS son la identificación de spam en  correos electrónicos (clasificación), como también la estimación del precio de una propiedad en función de su tamaño, ubicación y otras características (regresión) --- donde ambas aplicaciones requieren de un conjunto de entrenamiento construido por un humano. La segunda categoría es el aprendizaje no supervisado (AnS), en donde los datos no están etiquetados y el objetivo es encontrar estructura entre ellos, es decir, agrupar subconjuntos de datos que tienen algún grado de relación o propiedades en común, por ejemplo, el clustering de un gran número de artículos en distintas categorías basado en su frecuencia aparición de palabras \cite{salakhutdinov_2006}. La tercera categoría del AM es el aprendizaje reforzado (AR), en donde un agente aprende a tomar decisiones mediante la maximización de un funcional de recompensa, este es probablemente el tipo de aprendizaje más cercano a la forma en que los animales aprendemos, mediante prueba y error, como por ejemplo cuando entrenamos un perro para que aprenda algún truco recompensándolo con comida cada vez realiza la tarea correctamente. El reciente resultado de DeepMind donde una máquina aprendió a jugar Go usando una búsqueda de árbol y una red neural profunda es uno de los ejemplos más exitosos de este aprendizaje reforzado \cite{silver_2016}.


\subsection{Relación con otras disciplinas}

\textbf{AM y Estadística.} En el análisis de datos es posible identificar dos extremos: el aprendizaje inductivo en donde los datos son abundantes, los supuestos a priori sobre su naturaleza son vagos y el objetivo es realizar predicciones con algoritmos complejos que no necesariamente expliquen los datos. En el otro extremo está el aprendizaje deductivo, donde los datos no son masivos, se adoptan supuestos sobre su naturaleza y el objetivo es aprender relaciones significativas entre los datos usando modelos simples. Si bien en general al analizar datos nos movemos entre estos dos extremos, podemos decir que la estadística es más cercana al segundo extremo, mientras que el AM tiene componentes que son de enfoque deductivo y muy relacionado a estadística (inferencia bayesiana), y componentes de enfoque inductivo (redes neuronales). Una consecuencia de esto es la importancia que tienen los parámetros en distintos métodos de AM: En los métodos deductivos (estadísticos) los parámetros tienen un rol explicativo de la naturaleza del fenómeno en cuestión que es revelado por los datos, mientras que los métodos inductivos se caracterizan por tener una infinidad de parámetros sin una explicación clara, pues el objetivo muchas veces es directamente hacer predicciones. La relación entre estadística y algunos enfoques a AM es tan cercana que Robert Tibshirani se ha referido a AM como estadística pretenciosa (glorified statistics).

\textbf{AM y Programación Clásica.} La programación clásica construye algoritmos basados en reglas, es decir, una lista de instrucciones que son ejecutadas en una máquina, lo cual requiere que el programador conozca de antemano el algoritmo a programar, e.g., calcular la transformada de Fourier rápida (FFT) de una grabación de audio. Sin embargo, hay tareas en las cuales el algoritmo apropiado no es conocido, por lo tanto, el enfoque que adopta AM es programar directamente la máquina para que aprenda la lista de instrucciones. Tomemos el caso del ajedrez, de acuerdo a \cite{shannon_1950} el número de combinaciones posibles para el juego de ajedrez es del orden de $10^40$, esto significa que usando programación clásica un programa ingenuo para jugar ajedrez tendría que tener al menos esa cantidad de instrucciones de la forma “para la combinación C, ejecutar la acción A”. Si todos los humanos sobre la faz de la tierra se uniesen para programar esta rutina y cada uno pudiese escribir 10 de estas instrucciones por segundo, nos tomaría $4x10^{21}$ años, esto es casi un billón (1012) de veces la edad de la tierra ($4.54x10^9$), lo cual hace impracticable adoptar un enfoque clásico de programación. Una alternativa basada en AM es un programa simple en el cual la máquina explora distintos posibles escenarios del tablero e inicialmente toma decisiones aleatorias de qué acción ejecutar para luego registrar si dicha movida llevó a ganar o perder el juego, este enfoque de programación no pretende programar la máquina para “jugar ajedrez” sino para “aprender a jugar ajedrez”. Una exitosa implementación de este concepto usando aprendizaje reforzado y redes neuronales profundas para el juego de Go puede verse en \cite{silver_2016}.


\textbf{AM, Knowledge Discovery in Databases (KDD) y minería de datos.} KDD \cite{fayyad_1996} es “el proceso no trivial de identificar patrones potencialmente válidos, novedosos, útiles y explicativos en datos”, y consta de 5 etapas: selección, preparación, transformación, minería e interpretación de datos. La etapa de minería de datos  consiste en extraer información desde datos disponibles, por ejemplo, agruparlos en subconjuntos afines o identificar situaciones anómalas. Para esto se usan herramientas de AM, especialmente de aprendizaje no-supervisado debido a la cantidad de los datos, los cuales en general no están no etiquetados y existe poco conocimiento a priori de su naturaleza. 

\textbf{AM y Data Science.} En línea con sus orígenes en la inteligencia artificial, el objetivo del AM es encontrar relaciones entre datos y hacer predicciones prescindiendo de la intervención humana, es decir, con poco o nada de conocimiento a priori. Sin embargo, las técnicas de AM pueden ser complementadas con el conocimiento de un problema específico, en donde especialistas en AM colaboran con especialistas de (i) el área en cuestión, (ii) minería de datos, y (iii) visualización. Esto es Data Science, una disciplina colaborativa donde especialistas de variadas áreas unen fuerzas para hacer un análisis detallado de los datos, con la finalidad de resolver un problema en particular, usualmente con fines comerciales. El perfil del Data Scientist es muy completo, pues debe tener conocimiento de AM, estadística, programación, minería de datos, interactuar con especialistas y entender el impacto comercial del análisis, de hecho, Data Scientist ha sido considerado el puesto de trabajo más sexy del siglo XXI según Harvard Business Review.
\\
\todo[inline, color=blue!20]{Discusión: ¿Cómo aprendemos? hablar de over/underfitting}

\subsection{Estado del aprendizaje de máquinas y desafíos}

El aprendizaje de máquinas es una reciente disciplina que provee de herramientas a una gran cantidad de disciplinas, pero también es un área de investigación en sí misma con una activa comunidad y muchas preguntas abiertas. Desde el punto de vista algorítmico es posible identificar en primer lugar el desafío del entrenamiento en línea, es decir, ajustar el algoritmo cada vez que se dispone de un nuevo dato para operar continuamente (e.g. análisis de series de tiempo), este es un concepto fundamental en procesamiento adaptativo de señales y no ha sido tomado en cuenta satisfactoriamente aún en AM. Otro desafío que tiene relación con la implementación de algoritmos es la capacidad de escalar el entrenamiento de AM para Big Data,  pues en general los métodos de AM son costosos ya que se enfocan en descubrir aprendizaje y no en procesar datos de alta dimensión per se, sin embargo, esta habilidad es cada vez más necesaria en la era de la información donde, los conceptos que actualmente se usan en Big Data son básicos comparados con es estado del arte de AM.

También es posible identificar desafíos en un plano teórico, como por ejemplo la transferencia de aprendizaje, en donde la experiencia adquirida en la realización de una tarea (e.g., reconocimiento de automóviles) sirve como punto de partida para una tarea relacionada (e.g., reconocimiento de camiones), de la misma forma en que un físico o matemático es contratado para trabajar en un banco sin tener conocimiento a priori de finanzas. Por otro lado, un desafío ético son los riegos del uso de AM: el avance de AM parece a veces descontrolado y abre interrogantes con respecto de la legislación sobre el actuar de máquinas inteligentes. Por ejemplo, ¿quién es responsable en un accidente en el que está involucrado un automóvil autónomo? Estos últimos dos desafíos revelan que hay un gran área que no hemos explorado y que, a pesar de los avances teóricos y sobretodo aplicados del AM, estamos lejos de entender la inteligencia. Como ha sido expuesto en \cite{gal_2016}, nuestra relación con el entendimiento de la inteligencia mediante el uso del AM puede ser entendido como el Homo erectus hace 400.000 años frotando dos ramas para producir fuego, ellos usaron el fuego para abrigarse, cocinar y cazar, sin embargo, esto no quiere decir que entendían por qué al frotar dos ramas generaban fuego o, peor aún, qué es el fuego. Estamos en una etapa temprana del entendimiento del aprendizaje, en la que usamos estas herramientas “inteligentes” para nuestro bienestar, sin embargo, estamos lejos de entender la ciencia que hay detrás. 
    
